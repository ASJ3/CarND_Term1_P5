{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Vehicle Detection Project **\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier\n",
    "* Optionally, you can also apply a color transform and append binned color features, as well as histograms of color, to your HOG feature vector. \n",
    "* Note: for those first two steps don't forget to normalize your features and randomize a selection for training and testing.\n",
    "* Implement a sliding-window technique and use your trained classifier to search for vehicles in images.\n",
    "* Run your pipeline on a video stream (start with the test_video.mp4 and later implement on full project_video.mp4) and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.\n",
    "* Estimate a bounding box for vehicles detected.\n",
    "\n",
    "[//]: # (Image References)\n",
    "[image0]: ./examples/00HOG.png\n",
    "[image1]: ./examples/01variousBoxes.png\n",
    "[image2]: ./examples/02searchBoxes.png\n",
    "[image3]: ./examples/03initialPic.png\n",
    "[image4]: ./examples/04resize.png\n",
    "[image5]: ./examples/05HOG.png\n",
    "[image6]: ./examples/06heatmap.png\n",
    "[image7]: ./examples/07label.png\n",
    "[image8]: ./examples/08final.png\n",
    "[image9]: ./examples/09HSV.png\n",
    "[video1]: ./project_output.mp4\n",
    "\n",
    "## [Rubric](https://review.udacity.com/#!/rubrics/513/view) Points\n",
    "\n",
    "\n",
    "\n",
    "### Histogram of Oriented Gradients (HOG)\n",
    "\n",
    "#### 1. Explain how (and identify where in your code) you extracted HOG features from the training images.\n",
    "\n",
    "Histogram of Oriented Gradients was peformed on initial pictures from the training set, which are all 64x64 pixels.\n",
    "\n",
    "Here is a result of a vehicle image from the test set:\n",
    "\n",
    "![alt text][image0] \n",
    "\n",
    "Besides evaluating different HOG parameters, I also looked into different color spaces, such as HSL (see below).\n",
    "\n",
    "\n",
    "#### 2. Explain how you settled on your final choice of HOG parameters.\n",
    "\n",
    "The pictures were first covnerted to grayscale, then I tried various parameters of HOG to see if I could improve the accuracy of my Support Vector Machine trained on HOG. While having more orientations seemed to net better results, there is a price to pay in the sense that more orientations made training my SVM longer.\n",
    "I settled on the following:\n",
    "* Pixels per cell = 8\n",
    "* Cells per block = 2\n",
    "* Number of orientations = 9\n",
    "\n",
    "#### 3. Describe how (and identify where in your code) you trained a classifier using your selected HOG features.\n",
    "\n",
    "I trained a linear SVM using scikitlearn's GridSearchCV, which enabled me to compare different parameters and determine which ones were the best.\n",
    "\n",
    "I used an rbf kernel, and the following parameters:\n",
    "* C values: [0.1, 1, 10], \n",
    "* gamma values: [0.001, 0.01, 0.1]\n",
    "\n",
    "GridSearchCV found the best parameters were 10 for C and 0.1 for gamma.\n",
    "\n",
    "I then used these parameters to train the SVM on 80% of the training set, keeping 20% as a testing set, and got an accuracy of 98.99%\n",
    "\n",
    "### Sliding Window Search\n",
    "\n",
    "#### 1. Describe how (and identify where in your code) you implemented a sliding window search.  How did you decide what scales to search and how much to overlap windows?\n",
    "\n",
    "I spent a lot of time determining what window sizes I should use to scan the bottom half of an image (i.e. where cars are). Since training pictures were 64x64 pixels, I decided to use this size as the smallest and then go with multiples of it (i.e. 96, 128, and 192). \n",
    "\n",
    "![alt text][image1]\n",
    "\n",
    "Here is an example of what my scanning windows ultimately looked like (note: I actually used twice as many smaller windows, but I did not draw them on this image for readibility purposes)\n",
    "\n",
    "![alt text][image2]\n",
    "\n",
    "#### 2. Show some examples of test images to demonstrate how your pipeline is working.  What did you do to optimize the performance of your classifier?\n",
    "\n",
    "\n",
    "For the width, I decided to not use a square ratio but rather use a width that was about 130% of height, which I would then 'squeeze' when resizing the picture to 64x64 pixels. I did this because I realized cars are more wide than tall, and also because I could see many images on the training set had thus been squeezed on the x-axis, so the model had already been trained with such pictures.\n",
    "\n",
    "Here are the steps I took to draw boxes around cars on an image:\n",
    "1. crop the image to only keep the bottom half and convert it to grayscale\n",
    "\n",
    "![alt text][image3]\n",
    "\n",
    "2.  Apply sliding windows and resize the resulting images\n",
    "\n",
    "![alt text][image4]\n",
    "\n",
    "3. Get the Histogram of Oriented Gradient from the resized image and let the SVM predict if it contains a car\n",
    "\n",
    "![alt text][image5]\n",
    "\n",
    "4. Use all the overlapping bounding boxes (i.e. windows where the SVM model detected a car) and create a heat map\n",
    "\n",
    "![alt text][image6]\n",
    "\n",
    "4. Use a threshold to get rid of areas that were not selected in enough bounding boxes (to eliminate false positives). Use Python label() to determine areas where a car is positioned.\n",
    "\n",
    "![alt text][image7]\n",
    "\n",
    "6. Use the positions found to draw rectangles wich are superimposed on the original picture\n",
    "\n",
    "![alt text][image8]\n",
    "\n",
    "---\n",
    "\n",
    "### Video Implementation\n",
    "\n",
    "#### 1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (somewhat wobbly or unstable bounding boxes are ok as long as you are identifying the vehicles most of the time with minimal false positives.)\n",
    "Here's a [link to my video result](./project_output.mp4)\n",
    "\n",
    "\n",
    "#### 2. Describe how (and identify where in your code) you implemented some kind of filter for false positives and some method for combining overlapping bounding boxes.\n",
    "\n",
    "The biggest difference between the pipeline for the video and one for an individual picture, is that I kept the valid windows (i.e. the ones displayed on screen) from a few previous frames. This enabled me to smooth out the bounding box surrounding a car. I then adjusted my threshold so that I could give a higher value to the bounding boxes from the current frame, versus those from previous frames.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Discussion\n",
    "\n",
    "#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?\n",
    "The video took a fairly long time to process in it entirety, which made experimentation of parameter more difficult. I even tried to run my pipeline on the AWS EC2 instance I used in previous projects, but didn't see much of an improvement, which leads to think that the software method used is not optimized for parallel processing. This is a big problem in a real-word situation, where cars need to be recognized in real time.\n",
    "\n",
    "I also do not know how the SVM model would react under different wheather or lighting conditions (i.e. rain, snow, nightime), although I believe this could be solved by either using additional training data, or training different SVM models for different conditions (i.e. one model trained for night time).\n",
    "\n",
    "I would also like to explore further various color models (i.e. HSV, YCbCr) to see if they can help to further improve model accuracy and reduce false positives.\n",
    "\n",
    "![alt text][image9]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
